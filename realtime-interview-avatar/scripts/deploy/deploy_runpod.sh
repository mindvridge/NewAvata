#!/bin/bash

# ============================================================================
# RunPod Deployment Script for Interview Avatar System
# ============================================================================
# RunPod: https://www.runpod.io/
#
# ë¹„ìš© ì¶”ì • (ì‹œê°„ë‹¹):
#   - RTX 4090 (24GB):  $0.44 - $0.69 (On-Demand)
#   - RTX 4090 (24GB):  $0.34 - $0.54 (Spot)
#   - A100 (40GB):      $1.29 - $1.89 (On-Demand)
#   - A100 (40GB):      $0.99 - $1.49 (Spot)
#   - A100 (80GB):      $1.89 - $2.49 (On-Demand)
#
# ê¶Œìž¥: RTX 4090 Spot ($0.34/hr) - ë©´ì ‘ 1íšŒë‹¹ ì•½ $0.06-0.10
# ============================================================================

set -e

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê¹… í•¨ìˆ˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ë°°ë„ˆ ì¶œë ¥
print_banner() {
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘         RunPod Deployment - Interview Avatar              â•‘"
    echo "â•‘                                                            â•‘"
    echo "â•‘  GPU-accelerated realtime interview avatar deployment     â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
}

# ë¹„ìš© ì •ë³´ ì¶œë ¥
print_cost_estimate() {
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "                    ðŸ’° ë¹„ìš© ì¶”ì •                            "
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "GPU íƒ€ìž…ë³„ ì‹œê°„ë‹¹ ë¹„ìš©:"
    echo "  â€¢ RTX 4090 (24GB) Spot:      \$0.34 - \$0.54 /hr  â­ ê¶Œìž¥"
    echo "  â€¢ RTX 4090 (24GB) On-Demand: \$0.44 - \$0.69 /hr"
    echo "  â€¢ A100 (40GB) Spot:          \$0.99 - \$1.49 /hr"
    echo "  â€¢ A100 (40GB) On-Demand:     \$1.29 - \$1.89 /hr"
    echo "  â€¢ A100 (80GB) On-Demand:     \$1.89 - \$2.49 /hr"
    echo ""
    echo "ì˜ˆìƒ ì‚¬ìš©ëŸ‰:"
    echo "  â€¢ ë©´ì ‘ 1íšŒ (15ë¶„):           \$0.06 - \$0.10"
    echo "  â€¢ ì¼ì¼ 10íšŒ ë©´ì ‘:             \$0.60 - \$1.00"
    echo "  â€¢ ì›”ê°„ ìš´ì˜ (8ì‹œê°„/ì¼):       \$81.60 - \$129.60"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
}

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
check_env() {
    log_info "í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì¤‘..."

    if [ -z "$RUNPOD_API_KEY" ]; then
        log_error "RUNPOD_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        log_info "RunPod API í‚¤ë¥¼ ìƒì„±í•˜ì„¸ìš”: https://www.runpod.io/console/user/settings"
        exit 1
    fi

    # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
    if [ -f .env ]; then
        log_info ".env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ì¤‘..."
        source .env
    else
        log_warning ".env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. .env.exampleì„ ë³µì‚¬í•˜ì—¬ ì„¤ì •í•˜ì„¸ìš”."
    fi

    log_success "í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ"
}

# RunPod CLI ì„¤ì¹˜ í™•ì¸
check_runpod_cli() {
    log_info "RunPod CLI í™•ì¸ ì¤‘..."

    if ! command -v runpodctl &> /dev/null; then
        log_warning "RunPod CLIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘..."

        # OS ê°ì§€
        if [[ "$OSTYPE" == "linux-gnu"* ]]; then
            wget -qO- https://github.com/runpod/runpodctl/releases/latest/download/runpodctl-linux-amd64 -O /usr/local/bin/runpodctl
        elif [[ "$OSTYPE" == "darwin"* ]]; then
            wget -qO- https://github.com/runpod/runpodctl/releases/latest/download/runpodctl-darwin-amd64 -O /usr/local/bin/runpodctl
        else
            log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” OSìž…ë‹ˆë‹¤."
            exit 1
        fi

        chmod +x /usr/local/bin/runpodctl
        log_success "RunPod CLI ì„¤ì¹˜ ì™„ë£Œ"
    else
        log_success "RunPod CLI í™•ì¸ ì™„ë£Œ"
    fi

    # API í‚¤ ì„¤ì •
    runpodctl config --apiKey "$RUNPOD_API_KEY"
}

# GPU íƒ€ìž… ì„ íƒ
select_gpu() {
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "                    ðŸŽ® GPU íƒ€ìž… ì„ íƒ                        "
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "1) RTX 4090 (24GB) - Spot         â­ ê¶Œìž¥ (\$0.34-0.54/hr)"
    echo "2) RTX 4090 (24GB) - On-Demand    (\$0.44-0.69/hr)"
    echo "3) A100 (40GB) - Spot             (\$0.99-1.49/hr)"
    echo "4) A100 (40GB) - On-Demand        (\$1.29-1.89/hr)"
    echo "5) A100 (80GB) - On-Demand        (\$1.89-2.49/hr)"
    echo ""
    read -p "GPU íƒ€ìž…ì„ ì„ íƒí•˜ì„¸ìš” [1-5] (ê¸°ë³¸: 1): " gpu_choice

    gpu_choice=${gpu_choice:-1}

    case $gpu_choice in
        1)
            GPU_TYPE="NVIDIA GeForce RTX 4090"
            GPU_COUNT=1
            DISK_SIZE=50
            INSTANCE_TYPE="spot"
            ;;
        2)
            GPU_TYPE="NVIDIA GeForce RTX 4090"
            GPU_COUNT=1
            DISK_SIZE=50
            INSTANCE_TYPE="on-demand"
            ;;
        3)
            GPU_TYPE="NVIDIA A100-SXM4-40GB"
            GPU_COUNT=1
            DISK_SIZE=50
            INSTANCE_TYPE="spot"
            ;;
        4)
            GPU_TYPE="NVIDIA A100-SXM4-40GB"
            GPU_COUNT=1
            DISK_SIZE=50
            INSTANCE_TYPE="on-demand"
            ;;
        5)
            GPU_TYPE="NVIDIA A100-SXM4-80GB"
            GPU_COUNT=1
            DISK_SIZE=50
            INSTANCE_TYPE="on-demand"
            ;;
        *)
            log_error "ìž˜ëª»ëœ ì„ íƒìž…ë‹ˆë‹¤."
            exit 1
            ;;
    esac

    log_success "ì„ íƒëœ GPU: $GPU_TYPE ($INSTANCE_TYPE)"
}

# ë°°í¬ ëª¨ë“œ ì„ íƒ
select_deployment_mode() {
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "                  ðŸ“¦ ë°°í¬ ëª¨ë“œ ì„ íƒ                         "
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "1) Pod (ì§€ì†ì  ì‹¤í–‰)        - 24/7 ìš´ì˜, ê³ ì • ë¹„ìš©"
    echo "2) Serverless (ìš”ì²­ ê¸°ë°˜)  - ì‚¬ìš©ëŸ‰ ê¸°ë°˜, ìžë™ ìŠ¤ì¼€ì¼ë§"
    echo ""
    read -p "ë°°í¬ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” [1-2] (ê¸°ë³¸: 1): " mode_choice

    mode_choice=${mode_choice:-1}

    case $mode_choice in
        1)
            DEPLOYMENT_MODE="pod"
            ;;
        2)
            DEPLOYMENT_MODE="serverless"
            ;;
        *)
            log_error "ìž˜ëª»ëœ ì„ íƒìž…ë‹ˆë‹¤."
            exit 1
            ;;
    esac

    log_success "ì„ íƒëœ ë°°í¬ ëª¨ë“œ: $DEPLOYMENT_MODE"
}

# Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ
build_and_push_image() {
    log_info "Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."

    # Docker Hub ë¡œê·¸ì¸ í™•ì¸
    if [ -z "$DOCKER_USERNAME" ]; then
        log_error "DOCKER_USERNAME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        exit 1
    fi

    IMAGE_NAME="$DOCKER_USERNAME/interview-avatar:latest"

    # ì´ë¯¸ì§€ ë¹Œë“œ
    docker build -t "$IMAGE_NAME" -f docker/Dockerfile .

    log_info "Docker ì´ë¯¸ì§€ í‘¸ì‹œ ì¤‘..."
    docker push "$IMAGE_NAME"

    log_success "ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ ì™„ë£Œ: $IMAGE_NAME"
}

# Pod ë°°í¬
deploy_pod() {
    log_info "RunPod Pod ë°°í¬ ì¤‘..."

    POD_NAME="interview-avatar-$(date +%s)"

    # Pod ìƒì„±
    runpodctl create pod \
        --name "$POD_NAME" \
        --gpuType "$GPU_TYPE" \
        --gpuCount $GPU_COUNT \
        --containerDiskSize $DISK_SIZE \
        --imageName "$IMAGE_NAME" \
        --volumeSize 100 \
        --ports "8000/http" \
        --env "DEEPGRAM_API_KEY=$DEEPGRAM_API_KEY" \
        --env "ELEVENLABS_API_KEY=$ELEVENLABS_API_KEY" \
        --env "OPENAI_API_KEY=$OPENAI_API_KEY" \
        --env "DAILY_API_KEY=$DAILY_API_KEY" \
        --env "APP_ENV=production" \
        --env "LOG_LEVEL=INFO" \
        --env "NVIDIA_VISIBLE_DEVICES=all" \
        --env "CUDA_VISIBLE_DEVICES=0"

    log_success "Pod ë°°í¬ ì™„ë£Œ: $POD_NAME"

    # Pod ì •ë³´ ì¡°íšŒ
    sleep 5
    log_info "Pod ì •ë³´ ì¡°íšŒ ì¤‘..."
    runpodctl get pod "$POD_NAME"
}

# Serverless ë°°í¬
deploy_serverless() {
    log_info "RunPod Serverless ë°°í¬ ì¤‘..."

    ENDPOINT_NAME="interview-avatar-$(date +%s)"

    # Serverless ì—”ë“œí¬ì¸íŠ¸ ìƒì„± (RunPod API ì‚¬ìš©)
    curl -X POST "https://api.runpod.io/graphql" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $RUNPOD_API_KEY" \
        -d '{
            "query": "mutation { saveEndpoint(input: { name: \"'$ENDPOINT_NAME'\", gpuIds: \"'$GPU_TYPE'\", workersMin: 0, workersMax: 3, idleTimeout: 5, imageName: \"'$IMAGE_NAME'\", dockerArgs: \"-p 8000:8000\", env: [{ key: \"DEEPGRAM_API_KEY\", value: \"'$DEEPGRAM_API_KEY'\" }, { key: \"ELEVENLABS_API_KEY\", value: \"'$ELEVENLABS_API_KEY'\" }, { key: \"OPENAI_API_KEY\", value: \"'$OPENAI_API_KEY'\" }, { key: \"DAILY_API_KEY\", value: \"'$DAILY_API_KEY'\" }] }) { id name } }"
        }'

    log_success "Serverless ì—”ë“œí¬ì¸íŠ¸ ë°°í¬ ì™„ë£Œ: $ENDPOINT_NAME"
}

# í—¬ìŠ¤ì²´í¬
health_check() {
    log_info "í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰ ì¤‘..."

    if [ "$DEPLOYMENT_MODE" == "pod" ]; then
        # Pod URL ê°€ì ¸ì˜¤ê¸°
        POD_URL=$(runpodctl get pod "$POD_NAME" --json | jq -r '.httpEndpoint')

        # ìµœëŒ€ 5ë¶„ ëŒ€ê¸°
        for i in {1..30}; do
            if curl -f "$POD_URL/api/health" &> /dev/null; then
                log_success "í—¬ìŠ¤ì²´í¬ ì„±ê³µ!"
                log_success "ì• í”Œë¦¬ì¼€ì´ì…˜ URL: $POD_URL"
                return 0
            fi

            log_info "í—¬ìŠ¤ì²´í¬ ëŒ€ê¸° ì¤‘... ($i/30)"
            sleep 10
        done

        log_error "í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: íƒ€ìž„ì•„ì›ƒ"
        return 1
    fi
}

# ë°°í¬ ì •ë³´ ì €ìž¥
save_deployment_info() {
    log_info "ë°°í¬ ì •ë³´ ì €ìž¥ ì¤‘..."

    DEPLOYMENT_FILE="deployment_info_runpod_$(date +%Y%m%d_%H%M%S).json"

    cat > "$DEPLOYMENT_FILE" <<EOF
{
    "provider": "runpod",
    "deployment_mode": "$DEPLOYMENT_MODE",
    "pod_name": "$POD_NAME",
    "gpu_type": "$GPU_TYPE",
    "instance_type": "$INSTANCE_TYPE",
    "image_name": "$IMAGE_NAME",
    "deployed_at": "$(date -Iseconds)",
    "pod_url": "$POD_URL"
}
EOF

    log_success "ë°°í¬ ì •ë³´ ì €ìž¥ ì™„ë£Œ: $DEPLOYMENT_FILE"
}

# ë©”ì¸ í•¨ìˆ˜
main() {
    print_banner
    print_cost_estimate

    check_env
    check_runpod_cli
    select_gpu
    select_deployment_mode

    # Docker ì´ë¯¸ì§€ ë¹Œë“œ í™•ì¸
    read -p "Docker ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œ ë¹Œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/N]: " build_image
    if [[ "$build_image" =~ ^[Yy]$ ]]; then
        build_and_push_image
    fi

    # ë°°í¬ ì‹¤í–‰
    if [ "$DEPLOYMENT_MODE" == "pod" ]; then
        deploy_pod
        health_check
    else
        deploy_serverless
    fi

    save_deployment_info

    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    log_success "ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ðŸŽ‰"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""

    if [ "$DEPLOYMENT_MODE" == "pod" ]; then
        echo "Pod URL: $POD_URL"
        echo ""
        echo "ìœ ìš©í•œ ëª…ë ¹ì–´:"
        echo "  â€¢ Pod ìƒíƒœ í™•ì¸:  runpodctl get pod $POD_NAME"
        echo "  â€¢ Pod ë¡œê·¸ í™•ì¸:  runpodctl logs $POD_NAME"
        echo "  â€¢ Pod ì¤‘ì§€:       runpodctl stop pod $POD_NAME"
        echo "  â€¢ Pod ì‚­ì œ:       runpodctl remove pod $POD_NAME"
    fi

    echo ""
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
