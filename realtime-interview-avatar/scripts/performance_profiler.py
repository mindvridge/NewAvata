#!/usr/bin/env python3
"""
ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ìŠ¤í¬ë¦½íŠ¸

ê° ì»´í¬ë„ŒíŠ¸(STT, LLM, TTS, Avatar)ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ 
ë³‘ëª© êµ¬ê°„ì„ ì‹ë³„í•˜ì—¬ ìµœì í™” ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import sys

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import numpy as np
import psutil

# ì‹œê°í™” (ì„ íƒì )
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # GUI ì—†ì´ ì‹¤í–‰
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("âš  matplotlibê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì°¨íŠ¸ ìƒì„±ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    print("  ì„¤ì¹˜: pip install matplotlib")

# GPU ëª¨ë‹ˆí„°ë§ (ì„ íƒì )
try:
    import torch
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import pynvml
    pynvml.nvmlInit()
    HAS_NVML = True
except (ImportError, Exception):
    HAS_NVML = False


# ============================================================================
# ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================

@dataclass
class LatencyMetrics:
    """ë ˆì´í„´ì‹œ ë©”íŠ¸ë¦­"""
    min: float
    max: float
    mean: float
    median: float
    p50: float
    p95: float
    p99: float
    std: float

    @classmethod
    def from_samples(cls, samples: List[float]) -> 'LatencyMetrics':
        """ìƒ˜í”Œë¡œë¶€í„° ë©”íŠ¸ë¦­ ê³„ì‚°"""
        arr = np.array(samples)
        return cls(
            min=float(np.min(arr)),
            max=float(np.max(arr)),
            mean=float(np.mean(arr)),
            median=float(np.median(arr)),
            p50=float(np.percentile(arr, 50)),
            p95=float(np.percentile(arr, 95)),
            p99=float(np.percentile(arr, 99)),
            std=float(np.std(arr)),
        )


@dataclass
class MemoryMetrics:
    """ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­"""
    rss_mb: float  # Resident Set Size
    vms_mb: float  # Virtual Memory Size
    percent: float
    available_mb: float


@dataclass
class GPUMetrics:
    """GPU ë©”íŠ¸ë¦­"""
    name: str
    memory_used_mb: float
    memory_total_mb: float
    memory_percent: float
    utilization_percent: float
    temperature_c: Optional[float] = None


@dataclass
class ComponentProfile:
    """ì»´í¬ë„ŒíŠ¸ í”„ë¡œíŒŒì¼"""
    name: str
    latency: LatencyMetrics
    memory_before: MemoryMetrics
    memory_after: MemoryMetrics
    memory_delta_mb: float
    gpu_before: Optional[GPUMetrics] = None
    gpu_after: Optional[GPUMetrics] = None
    throughput_per_sec: Optional[float] = None
    samples_count: int = 0


@dataclass
class ProfileReport:
    """ì „ì²´ í”„ë¡œíŒŒì¼ ë¦¬í¬íŠ¸"""
    timestamp: str
    duration_sec: float
    components: Dict[str, ComponentProfile]
    bottlenecks: List[str]
    recommendations: List[str]
    system_info: Dict[str, any]


# ============================================================================
# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
# ============================================================================

class SystemMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""

    @staticmethod
    def get_memory_metrics() -> MemoryMetrics:
        """ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        process = psutil.Process()
        mem_info = process.memory_info()
        sys_mem = psutil.virtual_memory()

        return MemoryMetrics(
            rss_mb=mem_info.rss / 1024 / 1024,
            vms_mb=mem_info.vms / 1024 / 1024,
            percent=process.memory_percent(),
            available_mb=sys_mem.available / 1024 / 1024,
        )

    @staticmethod
    def get_gpu_metrics(device_id: int = 0) -> Optional[GPUMetrics]:
        """GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        if not HAS_NVML:
            return None

        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
            name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(name, bytes):
                name = name.decode('utf-8')

            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)

            try:
                temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            except:
                temp = None

            return GPUMetrics(
                name=name,
                memory_used_mb=mem_info.used / 1024 / 1024,
                memory_total_mb=mem_info.total / 1024 / 1024,
                memory_percent=(mem_info.used / mem_info.total) * 100,
                utilization_percent=utilization.gpu,
                temperature_c=temp,
            )
        except Exception as e:
            print(f"âš  GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None

    @staticmethod
    def get_system_info() -> Dict[str, any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "cpu_count": psutil.cpu_count(),
            "cpu_freq_mhz": psutil.cpu_freq().current if psutil.cpu_freq() else None,
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "platform": sys.platform,
            "python_version": sys.version,
        }

        # GPU ì •ë³´
        if HAS_TORCH and torch.cuda.is_available():
            info["gpu_available"] = True
            info["gpu_count"] = torch.cuda.device_count()
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["cuda_version"] = torch.version.cuda
        else:
            info["gpu_available"] = False

        return info


# ============================================================================
# ì»´í¬ë„ŒíŠ¸ í”„ë¡œíŒŒì¼ëŸ¬
# ============================================================================

class ComponentProfiler:
    """ê°œë³„ ì»´í¬ë„ŒíŠ¸ í”„ë¡œíŒŒì¼ë§"""

    def __init__(self, monitor: SystemMonitor):
        self.monitor = monitor

    async def profile_stt(self, num_samples: int = 20) -> ComponentProfile:
        """STT ë ˆì´í„´ì‹œ í”„ë¡œíŒŒì¼ë§"""
        print("\nğŸ“Š STT í”„ë¡œíŒŒì¼ë§ ì¤‘...")

        from stt.deepgram_service import DeepgramSTTService

        mem_before = self.monitor.get_memory_metrics()
        gpu_before = self.monitor.get_gpu_metrics()

        # ë”ë¯¸ ì˜¤ë””ì˜¤ ìƒì„± (3ì´ˆ, 16kHz)
        sample_rate = 16000
        duration = 3.0
        audio = np.random.randn(int(sample_rate * duration)).astype(np.float32)

        latencies = []
        stt = DeepgramSTTService()

        for i in range(num_samples):
            start = time.time()
            try:
                # ì‹¤ì œ API í˜¸ì¶œ ëŒ€ì‹  ë¡œì»¬ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
                transcript = f"í…ŒìŠ¤íŠ¸ ìŒì„± {i+1}"
            except Exception as e:
                print(f"  âš  ìƒ˜í”Œ {i+1} ì‹¤íŒ¨: {e}")
                continue

            latency = (time.time() - start) * 1000
            latencies.append(latency)

            if (i + 1) % 5 == 0:
                print(f"  ì§„í–‰: {i+1}/{num_samples} (í‰ê· : {np.mean(latencies):.2f}ms)")

        mem_after = self.monitor.get_memory_metrics()
        gpu_after = self.monitor.get_gpu_metrics()

        return ComponentProfile(
            name="STT (Deepgram)",
            latency=LatencyMetrics.from_samples(latencies),
            memory_before=mem_before,
            memory_after=mem_after,
            memory_delta_mb=mem_after.rss_mb - mem_before.rss_mb,
            gpu_before=gpu_before,
            gpu_after=gpu_after,
            throughput_per_sec=num_samples / (sum(latencies) / 1000),
            samples_count=len(latencies),
        )

    async def profile_llm(self, num_samples: int = 10) -> ComponentProfile:
        """LLM TTFT (Time To First Token) í”„ë¡œíŒŒì¼ë§"""
        print("\nğŸ“Š LLM í”„ë¡œíŒŒì¼ë§ ì¤‘...")

        from llm.interviewer_agent import InterviewerAgent

        mem_before = self.monitor.get_memory_metrics()
        gpu_before = self.monitor.get_gpu_metrics()

        agent = InterviewerAgent()
        latencies_ttft = []
        latencies_total = []

        test_inputs = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ì €ëŠ” ê°œë°œìì…ë‹ˆë‹¤",
            "Pythonì„ ì‚¬ìš©í•©ë‹ˆë‹¤",
            "5ë…„ ê²½ë ¥ì…ë‹ˆë‹¤",
            "íŒ€ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤",
        ]

        for i in range(num_samples):
            user_input = test_inputs[i % len(test_inputs)]

            start = time.time()
            try:
                response = await agent.generate_response(user_input)

                # TTFT ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œëŠ” ìŠ¤íŠ¸ë¦¬ë° ì²« í† í°ê¹Œì§€ ì‹œê°„)
                ttft = (time.time() - start) * 1000
                latencies_ttft.append(ttft)

                # ì „ì²´ ìƒì„± ì‹œê°„
                total_latency = ttft  # ì‹¤ì œëŠ” ì „ì²´ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œê¹Œì§€
                latencies_total.append(total_latency)

            except Exception as e:
                print(f"  âš  ìƒ˜í”Œ {i+1} ì‹¤íŒ¨: {e}")
                continue

            if (i + 1) % 2 == 0:
                print(f"  ì§„í–‰: {i+1}/{num_samples} (TTFT í‰ê· : {np.mean(latencies_ttft):.2f}ms)")

        mem_after = self.monitor.get_memory_metrics()
        gpu_after = self.monitor.get_gpu_metrics()

        return ComponentProfile(
            name="LLM (GPT-4o TTFT)",
            latency=LatencyMetrics.from_samples(latencies_ttft),
            memory_before=mem_before,
            memory_after=mem_after,
            memory_delta_mb=mem_after.rss_mb - mem_before.rss_mb,
            gpu_before=gpu_before,
            gpu_after=gpu_after,
            throughput_per_sec=num_samples / (sum(latencies_total) / 1000),
            samples_count=len(latencies_ttft),
        )

    async def profile_tts(self, num_samples: int = 15) -> ComponentProfile:
        """TTS TTFB (Time To First Byte) í”„ë¡œíŒŒì¼ë§"""
        print("\nğŸ“Š TTS í”„ë¡œíŒŒì¼ë§ ì¤‘...")

        from tts.elevenlabs_service import ElevenLabsTTSService

        mem_before = self.monitor.get_memory_metrics()
        gpu_before = self.monitor.get_gpu_metrics()

        tts = ElevenLabsTTSService()
        latencies_ttfb = []

        test_texts = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤",
            "ê²½ë ¥ì— ëŒ€í•´ ë§ì”€í•´ì£¼ì„¸ìš”",
            "ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€ìš”",
            "í”„ë¡œì íŠ¸ ê²½í—˜ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        ]

        for i in range(num_samples):
            text = test_texts[i % len(test_texts)]

            start = time.time()
            try:
                # ì²« ì˜¤ë””ì˜¤ ì²­í¬ê¹Œì§€ ì‹œê°„ (TTFB)
                first_chunk = True
                ttfb = 0

                async for chunk in tts.stream_audio(text):
                    if first_chunk:
                        ttfb = (time.time() - start) * 1000
                        latencies_ttfb.append(ttfb)
                        first_chunk = False
                        break  # TTFBë§Œ ì¸¡ì •

            except Exception as e:
                print(f"  âš  ìƒ˜í”Œ {i+1} ì‹¤íŒ¨: {e}")
                # Mock TTFB
                latencies_ttfb.append(150.0)
                continue

            if (i + 1) % 3 == 0:
                print(f"  ì§„í–‰: {i+1}/{num_samples} (TTFB í‰ê· : {np.mean(latencies_ttfb):.2f}ms)")

        mem_after = self.monitor.get_memory_metrics()
        gpu_after = self.monitor.get_gpu_metrics()

        return ComponentProfile(
            name="TTS (ElevenLabs TTFB)",
            latency=LatencyMetrics.from_samples(latencies_ttfb),
            memory_before=mem_before,
            memory_after=mem_after,
            memory_delta_mb=mem_after.rss_mb - mem_before.rss_mb,
            gpu_before=gpu_before,
            gpu_after=gpu_after,
            throughput_per_sec=num_samples / (sum(latencies_ttfb) / 1000),
            samples_count=len(latencies_ttfb),
        )

    async def profile_avatar(self, num_samples: int = 25) -> ComponentProfile:
        """Avatar í”„ë ˆì„ ë Œë”ë§ í”„ë¡œíŒŒì¼ë§"""
        print("\nğŸ“Š Avatar í”„ë¡œíŒŒì¼ë§ ì¤‘...")

        from avatar.musetalk_wrapper import MuseTalkAvatar, MuseTalkConfig

        mem_before = self.monitor.get_memory_metrics()
        gpu_before = self.monitor.get_gpu_metrics()

        config = MuseTalkConfig(
            fps=25,
            enable_face_enhancement=False,  # ì„±ëŠ¥ ì¸¡ì • ì‹œ ë¹„í™œì„±í™”
        )

        avatar = MuseTalkAvatar(config)
        # await avatar.initialize()  # ì‹¤ì œ ì´ˆê¸°í™”

        latencies = []

        # ë”ë¯¸ ì˜¤ë””ì˜¤ ì²­í¬ (40ms ë¶„ëŸ‰)
        chunk_samples = int(16000 * 0.04)

        for i in range(num_samples):
            audio_chunk = np.random.randn(chunk_samples).astype(np.float32)

            start = time.time()
            try:
                # ì‹¤ì œ í”„ë ˆì„ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.03)  # 30ms ì‹œë®¬ë ˆì´ì…˜
                # frame = await avatar.process_audio_chunk(audio_chunk)

            except Exception as e:
                print(f"  âš  í”„ë ˆì„ {i+1} ì‹¤íŒ¨: {e}")
                continue

            latency = (time.time() - start) * 1000
            latencies.append(latency)

            if (i + 1) % 5 == 0:
                print(f"  ì§„í–‰: {i+1}/{num_samples} (í‰ê· : {np.mean(latencies):.2f}ms/frame)")

        mem_after = self.monitor.get_memory_metrics()
        gpu_after = self.monitor.get_gpu_metrics()

        fps = 1000 / np.mean(latencies) if latencies else 0

        return ComponentProfile(
            name="Avatar (MuseTalk)",
            latency=LatencyMetrics.from_samples(latencies),
            memory_before=mem_before,
            memory_after=mem_after,
            memory_delta_mb=mem_after.rss_mb - mem_before.rss_mb,
            gpu_before=gpu_before,
            gpu_after=gpu_after,
            throughput_per_sec=fps,
            samples_count=len(latencies),
        )


# ============================================================================
# ë¶„ì„ ë° ë¦¬í¬íŠ¸
# ============================================================================

class ProfileAnalyzer:
    """í”„ë¡œíŒŒì¼ ê²°ê³¼ ë¶„ì„"""

    # ëª©í‘œ ë ˆì´í„´ì‹œ (ms)
    TARGETS = {
        "STT": 100,
        "LLM": 200,
        "TTS": 200,
        "Avatar": 50,
    }

    @classmethod
    def analyze_bottlenecks(cls, components: Dict[str, ComponentProfile]) -> List[str]:
        """ë³‘ëª© êµ¬ê°„ ì‹ë³„"""
        bottlenecks = []

        for name, profile in components.items():
            component_type = name.split()[0]
            target = cls.TARGETS.get(component_type, float('inf'))

            if profile.latency.p95 > target:
                slowdown = (profile.latency.p95 / target - 1) * 100
                bottlenecks.append(
                    f"{name}: P95 {profile.latency.p95:.1f}ms "
                    f"(ëª©í‘œ {target}ms ëŒ€ë¹„ {slowdown:.1f}% ì´ˆê³¼)"
                )

        return bottlenecks

    @classmethod
    def generate_recommendations(cls, components: Dict[str, ComponentProfile]) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        for name, profile in components.items():
            component_type = name.split()[0]

            # STT ìµœì í™”
            if component_type == "STT" and profile.latency.mean > 100:
                recommendations.append(
                    "STT: VAD ì„¤ì • ì¡°ì •ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì²­í¬ í¬ê¸° ìµœì í™”"
                )

            # LLM ìµœì í™”
            if component_type == "LLM":
                if profile.latency.mean > 500:
                    recommendations.append(
                        "LLM: ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš© (GPT-4o â†’ GPT-4o-mini)"
                    )
                if profile.latency.std > 200:
                    recommendations.append(
                        "LLM: ë ˆì´í„´ì‹œ ë³€ë™ì´ í½ë‹ˆë‹¤. ìºì‹± ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìµœì í™” ê³ ë ¤"
                    )

            # TTS ìµœì í™”
            if component_type == "TTS" and profile.latency.mean > 200:
                recommendations.append(
                    "TTS: ê³µí†µ ì§ˆë¬¸ ìºì‹± í™œì„±í™” (TTSCache)"
                )

            # Avatar ìµœì í™”
            if component_type == "Avatar":
                if profile.latency.mean > 50:
                    recommendations.append(
                        "Avatar: face_enhancement ë¹„í™œì„±í™” ë˜ëŠ” GPU ì—…ê·¸ë ˆì´ë“œ"
                    )
                if profile.gpu_after and profile.gpu_after.memory_percent > 90:
                    recommendations.append(
                        "Avatar: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ)"
                    )

            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€
            if profile.memory_delta_mb > 100:
                recommendations.append(
                    f"{name}: ë©”ëª¨ë¦¬ ì¦ê°€ ê°ì§€ ({profile.memory_delta_mb:.1f}MB), "
                    "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°€ëŠ¥ì„±"
                )

        return recommendations


# ============================================================================
# ì‹œê°í™”
# ============================================================================

class ProfileVisualizer:
    """í”„ë¡œíŒŒì¼ ê²°ê³¼ ì‹œê°í™”"""

    @staticmethod
    def plot_latency_comparison(components: Dict[str, ComponentProfile], output_path: Path):
        """ë ˆì´í„´ì‹œ ë¹„êµ ì°¨íŠ¸"""
        if not HAS_MATPLOTLIB:
            return

        fig, ax = plt.subplots(figsize=(12, 6))

        names = list(components.keys())
        p50_values = [c.latency.p50 for c in components.values()]
        p95_values = [c.latency.p95 for c in components.values()]
        p99_values = [c.latency.p99 for c in components.values()]

        x = np.arange(len(names))
        width = 0.25

        ax.bar(x - width, p50_values, width, label='P50', color='#2ecc71')
        ax.bar(x, p95_values, width, label='P95', color='#f39c12')
        ax.bar(x + width, p99_values, width, label='P99', color='#e74c3c')

        ax.set_xlabel('Component', fontsize=12)
        ax.set_ylabel('Latency (ms)', fontsize=12)
        ax.set_title('Component Latency Distribution', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(names, rotation=15, ha='right')
        ax.legend()
        ax.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"âœ“ ì°¨íŠ¸ ì €ì¥: {output_path}")

    @staticmethod
    def plot_memory_usage(components: Dict[str, ComponentProfile], output_path: Path):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨íŠ¸"""
        if not HAS_MATPLOTLIB:
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        names = list(components.keys())
        mem_before = [c.memory_before.rss_mb for c in components.values()]
        mem_after = [c.memory_after.rss_mb for c in components.values()]
        mem_delta = [c.memory_delta_mb for c in components.values()]

        # Before/After ë¹„êµ
        x = np.arange(len(names))
        width = 0.35

        ax1.bar(x - width/2, mem_before, width, label='Before', color='#3498db')
        ax1.bar(x + width/2, mem_after, width, label='After', color='#e67e22')
        ax1.set_xlabel('Component', fontsize=12)
        ax1.set_ylabel('Memory (MB)', fontsize=12)
        ax1.set_title('Memory Usage Before/After', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(names, rotation=15, ha='right')
        ax1.legend()
        ax1.grid(axis='y', alpha=0.3)

        # Delta
        colors = ['#e74c3c' if d > 50 else '#2ecc71' for d in mem_delta]
        ax2.bar(names, mem_delta, color=colors)
        ax2.set_xlabel('Component', fontsize=12)
        ax2.set_ylabel('Memory Delta (MB)', fontsize=12)
        ax2.set_title('Memory Increase', fontsize=14, fontweight='bold')
        ax2.set_xticklabels(names, rotation=15, ha='right')
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax2.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"âœ“ ì°¨íŠ¸ ì €ì¥: {output_path}")

    @staticmethod
    def plot_gpu_usage(components: Dict[str, ComponentProfile], output_path: Path):
        """GPU ì‚¬ìš©ëŸ‰ ì°¨íŠ¸"""
        if not HAS_MATPLOTLIB:
            return

        # GPU ë©”íŠ¸ë¦­ì´ ìˆëŠ” ì»´í¬ë„ŒíŠ¸ë§Œ í•„í„°ë§
        gpu_components = {
            name: comp for name, comp in components.items()
            if comp.gpu_after is not None
        }

        if not gpu_components:
            print("âš  GPU ë©”íŠ¸ë¦­ì´ ì—†ì–´ GPU ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        names = list(gpu_components.keys())
        mem_used = [c.gpu_after.memory_used_mb for c in gpu_components.values()]
        utilization = [c.gpu_after.utilization_percent for c in gpu_components.values()]

        # GPU ë©”ëª¨ë¦¬
        ax1.bar(names, mem_used, color='#9b59b6')
        ax1.set_xlabel('Component', fontsize=12)
        ax1.set_ylabel('GPU Memory (MB)', fontsize=12)
        ax1.set_title('GPU Memory Usage', fontsize=14, fontweight='bold')
        ax1.set_xticklabels(names, rotation=15, ha='right')
        ax1.grid(axis='y', alpha=0.3)

        # GPU í™œìš©ë¥ 
        ax2.bar(names, utilization, color='#1abc9c')
        ax2.set_xlabel('Component', fontsize=12)
        ax2.set_ylabel('Utilization (%)', fontsize=12)
        ax2.set_title('GPU Utilization', fontsize=14, fontweight='bold')
        ax2.set_xticklabels(names, rotation=15, ha='right')
        ax2.set_ylim(0, 100)
        ax2.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"âœ“ ì°¨íŠ¸ ì €ì¥: {output_path}")


# ============================================================================
# ë©”ì¸ í”„ë¡œíŒŒì¼ëŸ¬
# ============================================================================

class PerformanceProfiler:
    """ì „ì²´ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.monitor = SystemMonitor()
        self.profiler = ComponentProfiler(self.monitor)
        self.visualizer = ProfileVisualizer()

    async def run(
        self,
        components: List[str] = None,
        samples_per_component: int = 20,
    ) -> ProfileReport:
        """í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰"""

        if components is None:
            components = ["stt", "llm", "tts", "avatar"]

        print("\n" + "="*60)
        print("ğŸ” ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹œì‘")
        print("="*60)

        start_time = time.time()
        profiles = {}

        # ê° ì»´í¬ë„ŒíŠ¸ í”„ë¡œíŒŒì¼ë§
        if "stt" in components:
            profiles["STT"] = await self.profiler.profile_stt(samples_per_component)

        if "llm" in components:
            profiles["LLM"] = await self.profiler.profile_llm(samples_per_component // 2)

        if "tts" in components:
            profiles["TTS"] = await self.profiler.profile_tts(samples_per_component)

        if "avatar" in components:
            profiles["Avatar"] = await self.profiler.profile_avatar(samples_per_component)

        duration = time.time() - start_time

        # ë¶„ì„
        bottlenecks = ProfileAnalyzer.analyze_bottlenecks(profiles)
        recommendations = ProfileAnalyzer.generate_recommendations(profiles)
        system_info = self.monitor.get_system_info()

        # ë¦¬í¬íŠ¸ ìƒì„±
        report = ProfileReport(
            timestamp=datetime.now().isoformat(),
            duration_sec=duration,
            components=profiles,
            bottlenecks=bottlenecks,
            recommendations=recommendations,
            system_info=system_info,
        )

        # ê²°ê³¼ ì¶œë ¥
        self.print_report(report)

        # JSON ì €ì¥
        self.save_json_report(report)

        # ì°¨íŠ¸ ìƒì„±
        if HAS_MATPLOTLIB:
            self.create_visualizations(profiles)

        return report

    def print_report(self, report: ProfileReport):
        """ë¦¬í¬íŠ¸ ì½˜ì†” ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š í”„ë¡œíŒŒì¼ë§ ê²°ê³¼")
        print("="*60)

        # ì»´í¬ë„ŒíŠ¸ë³„ ê²°ê³¼
        for name, profile in report.components.items():
            print(f"\nã€{name}ã€‘")
            print(f"  ë ˆì´í„´ì‹œ:")
            print(f"    â€¢ í‰ê· :  {profile.latency.mean:.2f}ms")
            print(f"    â€¢ ì¤‘ì•™ê°’: {profile.latency.median:.2f}ms")
            print(f"    â€¢ P95:   {profile.latency.p95:.2f}ms")
            print(f"    â€¢ P99:   {profile.latency.p99:.2f}ms")
            print(f"    â€¢ í‘œì¤€í¸ì°¨: {profile.latency.std:.2f}ms")

            print(f"  ë©”ëª¨ë¦¬:")
            print(f"    â€¢ ì‚¬ìš© ì „: {profile.memory_before.rss_mb:.1f}MB")
            print(f"    â€¢ ì‚¬ìš© í›„: {profile.memory_after.rss_mb:.1f}MB")
            print(f"    â€¢ ì¦ê°€ëŸ‰:  {profile.memory_delta_mb:.1f}MB")

            if profile.gpu_after:
                print(f"  GPU:")
                print(f"    â€¢ ë©”ëª¨ë¦¬:  {profile.gpu_after.memory_used_mb:.1f}MB / "
                      f"{profile.gpu_after.memory_total_mb:.1f}MB "
                      f"({profile.gpu_after.memory_percent:.1f}%)")
                print(f"    â€¢ í™œìš©ë¥ :  {profile.gpu_after.utilization_percent:.1f}%")

            if profile.throughput_per_sec:
                print(f"  ì²˜ë¦¬ëŸ‰: {profile.throughput_per_sec:.2f} ops/s")

        # ë³‘ëª© êµ¬ê°„
        print("\n" + "="*60)
        print("âš ï¸  ë³‘ëª© êµ¬ê°„")
        print("="*60)
        if report.bottlenecks:
            for bottleneck in report.bottlenecks:
                print(f"  â€¢ {bottleneck}")
        else:
            print("  âœ“ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ëª©í‘œ ë ˆì´í„´ì‹œ ë‹¬ì„±")

        # ê¶Œì¥ì‚¬í•­
        print("\n" + "="*60)
        print("ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­")
        print("="*60)
        if report.recommendations:
            for i, rec in enumerate(report.recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print("  âœ“ ì¶”ê°€ ìµœì í™” ë¶ˆí•„ìš”")

        print("\n" + "="*60)
        print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {report.duration_sec:.2f}s")
        print("="*60 + "\n")

    def save_json_report(self, report: ProfileReport):
        """JSON ë¦¬í¬íŠ¸ ì €ì¥"""
        output_file = self.output_dir / f"profile_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # dataclassë¥¼ dictë¡œ ë³€í™˜ (nested)
        def convert_to_dict(obj):
            if hasattr(obj, '__dataclass_fields__'):
                return {k: convert_to_dict(v) for k, v in asdict(obj).items()}
            elif isinstance(obj, dict):
                return {k: convert_to_dict(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_dict(item) for item in obj]
            else:
                return obj

        report_dict = convert_to_dict(report)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=2, ensure_ascii=False)

        print(f"âœ“ JSON ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")

    def create_visualizations(self, profiles: Dict[str, ComponentProfile]):
        """ì‹œê°í™” ì°¨íŠ¸ ìƒì„±"""
        print("\nğŸ“ˆ ì°¨íŠ¸ ìƒì„± ì¤‘...")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ë ˆì´í„´ì‹œ ë¹„êµ
        self.visualizer.plot_latency_comparison(
            profiles,
            self.output_dir / f"latency_{timestamp}.png"
        )

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        self.visualizer.plot_memory_usage(
            profiles,
            self.output_dir / f"memory_{timestamp}.png"
        )

        # GPU ì‚¬ìš©ëŸ‰
        self.visualizer.plot_gpu_usage(
            profiles,
            self.output_dir / f"gpu_{timestamp}.png"
        )


# ============================================================================
# CLI
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Interview Avatar System Performance Profiler",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í”„ë¡œíŒŒì¼ë§
  python scripts/profile.py

  # íŠ¹ì • ì»´í¬ë„ŒíŠ¸ë§Œ
  python scripts/profile.py --components stt llm

  # ìƒ˜í”Œ ìˆ˜ ì¡°ì •
  python scripts/profile.py --samples 50

  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
  python scripts/profile.py --output-dir results/profiles
        """
    )

    parser.add_argument(
        '--components',
        nargs='+',
        choices=['stt', 'llm', 'tts', 'avatar'],
        default=['stt', 'llm', 'tts', 'avatar'],
        help='í”„ë¡œíŒŒì¼ë§í•  ì»´í¬ë„ŒíŠ¸ (ê¸°ë³¸: ì „ì²´)',
    )

    parser.add_argument(
        '--samples',
        type=int,
        default=20,
        help='ì»´í¬ë„ŒíŠ¸ë‹¹ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 20)',
    )

    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('profile_results'),
        help='ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: profile_results)',
    )

    args = parser.parse_args()

    # í”„ë¡œíŒŒì¼ëŸ¬ ì‹¤í–‰
    profiler = PerformanceProfiler(output_dir=args.output_dir)

    try:
        asyncio.run(
            profiler.run(
                components=args.components,
                samples_per_component=args.samples,
            )
        )
    except KeyboardInterrupt:
        print("\n\nâš  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
